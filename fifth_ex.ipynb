{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee5242e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "b935156b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "unmasker_bert = pipeline('fill-mask', model='bert-base-multilingual-cased')\n",
    "unmasker_roberta = pipeline('fill-mask', model='xlm-roberta-base')\n",
    "unmasker_distilbert = pipeline('fill-mask', model='distilbert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "120ba417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.4260772466659546,\n",
       "  'token': 10114,\n",
       "  'token_str': 'to',\n",
       "  'sequence': 'Cześć jestem to model.'},\n",
       " {'score': 0.0634026974439621,\n",
       "  'token': 118,\n",
       "  'token_str': '-',\n",
       "  'sequence': 'Cześć jestem - model.'},\n",
       " {'score': 0.06284543871879578,\n",
       "  'token': 117,\n",
       "  'token_str': ',',\n",
       "  'sequence': 'Cześć jestem, model.'},\n",
       " {'score': 0.0355927012860775,\n",
       "  'token': 10701,\n",
       "  'token_str': 'jako',\n",
       "  'sequence': 'Cześć jestem jako model.'},\n",
       " {'score': 0.031229058280587196,\n",
       "  'token': 119,\n",
       "  'token_str': '.',\n",
       "  'sequence': 'Cześć jestem. model.'}]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker_bert(\"Cześć jestem [MASK] model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "72c1d5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result(text: str) -> None:\n",
    "    print(f\"{'bert-base-multilingual-cased':<30}{'|':<2}{'xlm-roberta-base':<25}{'|':<2}{'distilbert-base-multilingual-cased'}\")\n",
    "    bert_res = unmasker_bert(text)\n",
    "    roberta_res = unmasker_roberta(text.replace(\"[MASK]\", \"<mask>\"))\n",
    "    distilbert_res = unmasker_distilbert(text)\n",
    "    for bert, xlm, distilbert in zip(bert_res, roberta_res, distilbert_res):\n",
    "        print(f\"{round(bert['score'],2):<5} : {bert['token_str']:<21} {'|':<1} {round(xlm['score'],2):<5} : {xlm['token_str']:<16} {'|'} {round(distilbert['score'],2):<5} : {distilbert['token_str']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "6b30b720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-multilingual-cased  | xlm-roberta-base         | distilbert-base-multilingual-cased\n",
      "0.51  : .                     | 0.06  : online           | 0.24  : :\n",
      "0.12  : ,                     | 0.04  : .                | 0.11  : ?\n",
      "0.04  : :                     | 0.04  : tutaj            | 0.03  : !\n",
      "0.03  : !                     | 0.04  : ...              | 0.02  : .\n",
      "0.02  : -                     | 0.02  : ...              | 0.01  : ##em\n"
     ]
    }
   ],
   "source": [
    "show_result(\"Jestem [MASK]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f3dcee",
   "metadata": {},
   "source": [
    "# Devise a method to test if the langage model understands Polish cases. E.g. testing for nominal case could be expressed as \"Warszawa to największe [MASK]\", and the masked word should be in nominative case. Create sentences for each case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e93f3a",
   "metadata": {},
   "source": [
    "## Mianownik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "d3ea5335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-multilingual-cased  | xlm-roberta-base         | distilbert-base-multilingual-cased\n",
      "0.62  : .                     | 0.3   : ...              | 0.56  : miasto\n",
      "0.16  : miasto                | 0.16  : miasto           | 0.1   : miasta\n",
      "0.03  : Miasto                | 0.07  : !                | 0.02  : Miasto\n",
      "0.02  : miasta                | 0.05  : ...              | 0.01  : .\n",
      "0.01  : :                     | 0.05  : city             | 0.01  : ##sza\n"
     ]
    }
   ],
   "source": [
    "show_result(\"Warszawa to największe [MASK]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045044af",
   "metadata": {},
   "source": [
    "## Dopełniacz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "e9388c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-multilingual-cased  | xlm-roberta-base         | distilbert-base-multilingual-cased\n",
      "0.89  : .                     | 0.45  : nic              | 0.21  : ?\n",
      "0.03  : ,                     | 0.11  : .                | 0.14  : .\n",
      "0.02  : ;                     | 0.08  : zdjęcia          | 0.1   : !\n",
      "0.01  : :                     | 0.06  : ...              | 0.03  : :\n",
      "0.0   : !                     | 0.06  : zdjęć            | 0.03  : ;\n"
     ]
    }
   ],
   "source": [
    "show_result(\"Kasia nie zrobiła [MASK]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1141776",
   "metadata": {},
   "source": [
    "## Celownik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "cc39568e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-multilingual-cased  | xlm-roberta-base         | distilbert-base-multilingual-cased\n",
      "0.09  : nawet                 | 0.24  : jak              | 0.02  : tylko\n",
      "0.09  : tylko                 | 0.07  : nawet            | 0.02  : wszystkim\n",
      "0.06  : nie                   | 0.05  : już              | 0.02  : 丫\n",
      "0.05  : również               | 0.05  : tylko            | 0.02  : pracy\n",
      "0.04  : także                 | 0.05  : ostatnio         | 0.01  : miejsca\n"
     ]
    }
   ],
   "source": [
    "show_result(\"Poskarżyłem się [MASK] na nieuprzejmego klienta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fa3fec",
   "metadata": {},
   "source": [
    "## Biernik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "0697e79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-multilingual-cased  | xlm-roberta-base         | distilbert-base-multilingual-cased\n",
      "0.53  : .                     | 0.67  : dziecko          | 0.43  : dzieci\n",
      "0.02  : ;                     | 0.09  : dzieci           | 0.03  : :\n",
      "0.02  : ,                     | 0.06  : j                | 0.03  : utanas\n",
      "0.02  : dni                   | 0.02  : mu               | 0.02  : .\n",
      "0.02  : ...                   | 0.02  : go               | 0.02  : ২০০১\n"
     ]
    }
   ],
   "source": [
    "show_result(\"Matka usypia małe [MASK]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0adec5c",
   "metadata": {},
   "source": [
    "## Narzędnik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "43499105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-multilingual-cased  | xlm-roberta-base         | distilbert-base-multilingual-cased\n",
      "0.35  : nią                   | 0.33  : dziećmi          | 0.06  : innymi\n",
      "0.05  : nim                   | 0.21  : dzieckiem        | 0.04  : siebie\n",
      "0.04  : żoną                  | 0.19  : ...              | 0.04  : nim\n",
      "0.04  : tym                   | 0.03  : nią              | 0.03  : :\n",
      "0.02  : którą                 | 0.03  : ...              | 0.03  : nimi\n"
     ]
    }
   ],
   "source": [
    "show_result(\"Idę sobie na spacer razem z [MASK]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fa587d",
   "metadata": {},
   "source": [
    "## Miejscownik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "30030ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-multilingual-cased  | xlm-roberta-base         | distilbert-base-multilingual-cased\n",
      "0.12  : tym                   | 0.04  : 12               | 0.34  : 1596\n",
      "0.04  : to                    | 0.04  : 10               | 0.06  : ##czu\n",
      "0.03  : co                    | 0.03  : tym              | 0.03  : ziemi\n",
      "0.03  : północy               | 0.03  : 11               | 0.02  : Ziemi\n",
      "0.02  : tom                   | 0.02  : 6                | 0.01  : wiele\n"
     ]
    }
   ],
   "source": [
    "show_result(\"Rozmawialiśmy wczoraj o [MASK]. Ma jutro padać.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c8c262",
   "metadata": {},
   "source": [
    "## Wołacz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "cd1da263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-multilingual-cased  | xlm-roberta-base         | distilbert-base-multilingual-cased\n",
      "0.3   : !                     | 0.12  : Hej              | 0.02  : !\n",
      "0.02  : -                     | 0.04  : Super            | 0.01  : Oh\n",
      "0.01  : ...                   | 0.02  : Ah               | 0.01  : Polonia\n",
      "0.01  : ¡                     | 0.02  : Wow              | 0.01  : Nie\n",
      "0.01  : Pan                   | 0.02  : OK               | 0.01  : Pan\n"
     ]
    }
   ],
   "source": [
    "show_result(\"[MASK]! Czas na obiad!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcc1873",
   "metadata": {},
   "source": [
    "# Devise a method to test long-range relationships such as gender. E.e. you can use two verbs where withe masculine and feminine gender, where one of the verbs is masked. Both verbs should have the same gender, assuming the subject is the same. Define at least 3 such sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "3458399c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-multilingual-cased  | xlm-roberta-base         | distilbert-base-multilingual-cased\n",
      "0.28  : to                    | 0.36  : wiem             | 0.12  : ἶ\n",
      "0.11  : tego                  | 0.09  : myślę            | 0.1   : tego\n",
      "0.06  : pokazuje              | 0.09  : wiedział         | 0.02  : ##няя\n",
      "0.02  : jest                  | 0.07  : widać            | 0.02  : jest\n",
      "0.02  : ,                     | 0.06  : stwierdził       | 0.02  : to\n"
     ]
    }
   ],
   "source": [
    "show_result(\"Byłam tam razem z nim. Od początku [MASK], że coś jest nie tak\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "df15dd23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-multilingual-cased  | xlm-roberta-base         | distilbert-base-multilingual-cased\n",
      "0.48  : na                    | 0.45  : miałem           | 0.12  : przez\n",
      "0.06  : to                    | 0.12  : chciał           | 0.04  : ich\n",
      "0.05  : za                    | 0.09  : musiał           | 0.04  : gdy\n",
      "0.02  : przez                 | 0.05  : mieliśmy         | 0.04  : jak\n",
      "0.02  : on                    | 0.05  : miałam           | 0.02  : na\n"
     ]
    }
   ],
   "source": [
    "show_result(\"Rozwiązywałem zadanie domowe, a potem [MASK] coś zjeść\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "b2199563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-multilingual-cased  | xlm-roberta-base         | distilbert-base-multilingual-cased\n",
      "0.68  : ##my                  | 0.83  : ją               | 0.09  : sobie\n",
      "0.04  : ##y                   | 0.13  : jej              | 0.08  : ich\n",
      "0.03  : ##ś                   | 0.01  : je               | 0.06  : sie\n",
      "0.03  : mu                    | 0.01  : kogoś            | 0.05  : było\n",
      "0.02  : ##mu                  | 0.0   : nią              | 0.03  : się\n"
     ]
    }
   ],
   "source": [
    "show_result(\"Wspólnie podrózowaliśmy po Europie. Zawsze kiedy prosiłem [MASK] o pomoc, była przy mnie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e23eeb",
   "metadata": {},
   "source": [
    "# Check if the model captures real-world knolwedge. For instance a sentence \"[MASK] wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza.\" checks if the model \"knows\" the description of water. Define at least 3 such sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "2771d854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-multilingual-cased  | xlm-roberta-base         | distilbert-base-multilingual-cased\n",
      "0.04  : Jego                  | 0.11  : Najlepiej        | 0.09  : Na\n",
      "0.03  : Za                    | 0.1   : -                | 0.05  : We\n",
      "0.03  : Po                    | 0.04  : Najczęściej      | 0.03  : Od\n",
      "0.02  : Na                    | 0.04  : –                | 0.03  : W\n",
      "0.02  : W                     | 0.03  : Następnie        | 0.03  : we\n"
     ]
    }
   ],
   "source": [
    "show_result(\"[MASK] wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "a456201d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-multilingual-cased  | xlm-roberta-base         | distilbert-base-multilingual-cased\n",
      "0.1   : nie                   | 0.03  : Strona           | 0.07  : Film\n",
      "0.09  : ma                    | 0.02  : Nie              | 0.04  : Program\n",
      "0.04  : -                     | 0.02  : •                | 0.03  : Miasto\n",
      "0.03  : Ma                    | 0.01  : -                | 0.03  : Muzeum\n",
      "0.02  : życie                 | 0.01  : Ona              | 0.02  : Parafia\n"
     ]
    }
   ],
   "source": [
    "show_result(\"Temepratura słońca wynosi [MASK] stopni Celsjusza\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "e12929d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-multilingual-cased  | xlm-roberta-base         | distilbert-base-multilingual-cased\n",
      "0.06  : 6                     | 0.07  : 365              | 0.05  : kilka\n",
      "0.04  : 3                     | 0.04  : 180              | 0.02  : trzy\n",
      "0.03  : 4                     | 0.04  : 90               | 0.02  : 3\n",
      "0.03  : 5                     | 0.03  : 30               | 0.01  : 6\n",
      "0.03  : 7                     | 0.03  : 14               | 0.01  : cztery\n"
     ]
    }
   ],
   "source": [
    "show_result(\"Rok przestępny ma [MASK] dni\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970b7afc",
   "metadata": {},
   "source": [
    "## Answer the following questions:\n",
    "\n",
    "###    Which of the models produced the best results?\n",
    "Biorąc pod uwagę wszystkie zadania najlepiej poradził sobie model XLM\n",
    "### Was any of the models able to capture Polish grammar?\n",
    "Żaden model w 100% nie poradził sobie z tym zadaniem. Najlepiej poszło XLM, po nim distiblert a na końcu bert\n",
    "### Was any of the models able to capture long-distant relationships between the words?\n",
    "Tak. Model XLM dał sobie radę.\n",
    "### Was any of the models able to capture world knowledge?\n",
    "Niestety, ale tylko w jednym przypadku poradził sobie model XLM\n",
    "### What are the most striking errors made by the models?\n",
    "Wyrzucanie artefaktów typu ##няя i wstawianie wyrazu z dużej litery w środku zdania np. Film, Program, Miasto, Muzeum, Parafia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947ca3bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
